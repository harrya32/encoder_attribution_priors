{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22160,"status":"ok","timestamp":1702979789420,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"3XPm3DUU3yGb","outputId":"4e335da6-c5d8-458e-fe28-aa17b495c733"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: captum in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.7.0)\n","Requirement already satisfied: torch>=1.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from captum) (2.0.1)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from captum) (1.24.4)\n","Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from captum) (3.2.1)\n","Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from captum) (4.61.0)\n","Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch>=1.6->captum) (1.11.1)\n","Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch>=1.6->captum) (3.10.7)\n","Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch>=1.6->captum) (3.0)\n","Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch>=1.6->captum) (3.10.0.0)\n","Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch>=1.6->captum) (3.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from jinja2->torch>=1.6->captum) (2.0.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->captum) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->captum) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->captum) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->captum) (0.10.0)\n","Requirement already satisfied: six in /Users/harryamad/Library/Python/3.8/lib/python/site-packages (from cycler>=0.10->matplotlib->captum) (1.14.0)\n","Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","\u001b[33mWARNING: You are using pip version 21.2.1; however, version 23.3.1 is available.\n","You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n","Processing /Users/harryamad/Google Drive/encoder_attribution_priors\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (2.0.1)\n","Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (0.15.2)\n","Requirement already satisfied: captum in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (0.7.0)\n","Requirement already satisfied: hydra-core in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (1.3.2)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (1.24.4)\n","Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (1.3.1)\n","Requirement already satisfied: scikit-image in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (0.21.0)\n","Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (1.2.2)\n","Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (1.10.1)\n","Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (0.11.1)\n","Requirement already satisfied: tabulate in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (0.9.0)\n","Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (4.61.0)\n","Requirement already satisfied: wget in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (3.2)\n","Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from lfxai==0.1.1) (3.2.1)\n","Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->lfxai==0.1.1) (3.0.1)\n","Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->lfxai==0.1.1) (3.10.0.0)\n","Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->lfxai==0.1.1) (3.0)\n","Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->lfxai==0.1.1) (1.11.1)\n","Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torch->lfxai==0.1.1) (3.10.7)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from hydra-core->lfxai==0.1.1) (4.9.3)\n","Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from hydra-core->lfxai==0.1.1) (23.2)\n","Requirement already satisfied: importlib-resources in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from hydra-core->lfxai==0.1.1) (6.1.1)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from hydra-core->lfxai==0.1.1) (2.3.0)\n","Requirement already satisfied: PyYAML>=5.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from omegaconf<2.4,>=2.2->hydra-core->lfxai==0.1.1) (5.4.1)\n","Requirement already satisfied: zipp>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from importlib-resources->hydra-core->lfxai==0.1.1) (3.15.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from jinja2->torch->lfxai==0.1.1) (2.0.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->lfxai==0.1.1) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->lfxai==0.1.1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->lfxai==0.1.1) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib->lfxai==0.1.1) (2.8.1)\n","Requirement already satisfied: six in /Users/harryamad/Library/Python/3.8/lib/python/site-packages (from cycler>=0.10->matplotlib->lfxai==0.1.1) (1.14.0)\n","Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->lfxai==0.1.1) (2019.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-image->lfxai==0.1.1) (1.4.1)\n","Requirement already satisfied: pillow>=9.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-image->lfxai==0.1.1) (9.5.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-image->lfxai==0.1.1) (2023.7.10)\n","Requirement already satisfied: imageio>=2.27 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-image->lfxai==0.1.1) (2.31.1)\n","Requirement already satisfied: lazy_loader>=0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-image->lfxai==0.1.1) (0.3)\n","Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->lfxai==0.1.1) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->lfxai==0.1.1) (2.1.0)\n","Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sympy->torch->lfxai==0.1.1) (1.3.0)\n","Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from torchvision->lfxai==0.1.1) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchvision->lfxai==0.1.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchvision->lfxai==0.1.1) (1.25.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchvision->lfxai==0.1.1) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->torchvision->lfxai==0.1.1) (2.9)\n","Building wheels for collected packages: lfxai\n","  Building wheel for lfxai (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for lfxai: filename=lfxai-0.1.1-py3-none-any.whl size=30792 sha256=417c43802d7e1f79f5c898207f46534ed0657737d75baef88c7fadfb24560f8e\n","  Stored in directory: /private/var/folders/v1/8lvgykdd7jz1mgwv6lgglt540000gn/T/pip-ephem-wheel-cache-u10dc0ig/wheels/d8/ad/fa/7f23a7351b78cbce54840af97b785c69f88ecbff5d524953e6\n","Successfully built lfxai\n","Installing collected packages: lfxai\n","  Attempting uninstall: lfxai\n","    Found existing installation: lfxai 0.1.1\n","    Uninstalling lfxai-0.1.1:\n","      Successfully uninstalled lfxai-0.1.1\n","Successfully installed lfxai-0.1.1\n","\u001b[33mWARNING: You are using pip version 21.2.1; however, version 23.3.1 is available.\n","You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["!pip install captum\n","%pip install ."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6411,"status":"ok","timestamp":1702979795828,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"z5BzCW00v06E"},"outputs":[],"source":["import argparse\n","import csv\n","import itertools\n","import logging\n","import os\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torchvision\n","from captum.attr import GradientShap, IntegratedGradients, Saliency\n","from scipy.stats import spearmanr\n","from torch.utils.data import DataLoader, RandomSampler, Subset\n","from torchvision import transforms\n","\n","from lfxai.explanations.examples import (\n","    InfluenceFunctions,\n","    NearestNeighbours,\n","    SimplEx,\n","    TracIn,\n",")\n","from lfxai.explanations.features import attribute_auxiliary, attribute_individual_dim, tensor_attribution, attribute_training\n","from lfxai.models.images import (\n","    VAE,\n","    AutoEncoderMnist,\n","    ClassifierMnist,\n","    DecoderBurgess,\n","    DecoderMnist,\n","    EncoderBurgess,\n","    EncoderMnist,\n",")\n","from lfxai.models.losses import BetaHLoss, BtcvaeLoss, EntropyLoss, PearsonLoss\n","from lfxai.models.pretext import Identity, Mask, RandomNoise\n","from lfxai.utils.datasets import MaskedMNIST\n","from lfxai.utils.feature_attribution import generate_masks\n","from lfxai.utils.metrics import (\n","    compute_metrics,\n","    cos_saliency,\n","    count_activated_neurons,\n","    entropy_saliency_tensor,\n","    entropy_saliency,\n","    pearson_saliency,\n","    similarity_rates,\n","    spearman_saliency,\n","    pearson_saliency_tensor\n",")\n","from lfxai.utils.visualize import (\n","    correlation_latex_table,\n","    plot_pretext_saliencies,\n","    plot_pretext_top_example,\n","    plot_vae_saliencies,\n","    vae_box_plots,\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["a = torch.tensor([[-11],[2]])\n","b = torch.tensor([[3],[4]])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1, 3],\n","        [2, 4]])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["torch.cat([a,b], axis=1)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[11],\n","        [ 2]])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["torch.abs(a)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"data":{"text/plain":["array([[[[1]],\n","\n","        [[2]],\n","\n","        [[3]]],\n","\n","\n","       [[[4]],\n","\n","        [[5]],\n","\n","        [[6]]]])"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["latents = []\n","latents.append(torch.tensor([[1,2,3], [4,5,6]]).detach().cpu().numpy())\n","latents = np.concatenate(latents)\n","latents = np.expand_dims(latents, (2, 3))\n","latents"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[[1]],\n","\n","         [[2]],\n","\n","         [[3]]],\n","\n","\n","        [[[4]],\n","\n","         [[5]],\n","\n","         [[6]]]])"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["latents = []\n","latents.append(torch.tensor([[1,2,3], [4,5,6]]))\n","latents = torch.cat(latents)\n","latents = torch.unsqueeze(torch.unsqueeze(latents, 2), 2)\n","latents"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def test_attribution(\n","    encoder: callable,\n","    dim_latent: int,\n","    data_loader: torch.utils.data.DataLoader,\n","    device: torch.device,\n","    attr_method,\n","    baseline: torch.Tensor,\n","):\n","\n","    attributions = []\n","    latents = []\n","    for input_batch, _ in data_loader:\n","        input_batch = input_batch.to(device)\n","        attributions_batch = []\n","        latents.append(encoder(input_batch))\n","        for dim in range(dim_latent):\n","            attribution = (attr_method.attribute(input_batch, baseline, target=dim))\n","            attributions_batch.append(attribution)\n","        attributions.append(torch.cat(attributions_batch, axis=1))\n","    \n","    latents = torch.cat(latents)\n","    attributions = torch.cat(attributions)\n","    attributions = torch.abs(torch.unsqueeze(torch.unsqueeze(latents, 2), 2) * attributions)\n","    return attributions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["W=32\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","data_dir = Path.cwd() / \"data/mnist\"\n","\n","img_size = (1, W, W)\n","encoder = EncoderBurgess(img_size, 3)\n","\n","baseline_image = torch.zeros((1, 1, W, W), device=device)\n","test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","\n","\n","test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","test_dataset.transform = test_transform\n","test_dataset.data, test_dataset.targets = test_dataset.data[[1,2,3,4,5]], test_dataset.targets[[1,2,3,4,5]]\n","test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False)\n","\n","gradshap = GradientShap(encoder.mu)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["saliency_tensor = tensor_attribution(encoder.mu, 3, test_loader, device, gradshap, baseline_image) \n","saliency_array = attribute_individual_dim(encoder.mu, 3, test_loader, device, gradshap, baseline_image)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["def off_diagonal_sum(mat: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Computes the sum of of-diagonal matrix elements\n","    Args:\n","        mat: matrix\n","\n","    Returns:\n","        sum of the off diagonal elements of mat\n","    \"\"\"\n","    return np.sum(mat) - np.trace(mat)\n","def pearson_saliency(saliency: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Computes the average Pearson correlation between different saliency maps\n","    Args:\n","        saliency: saliency maps stacked together (indexed by the first tensor dimension)\n","\n","    Returns:\n","        Pearson correlation between saliency maps\n","    \"\"\"\n","    latent_dim = saliency.shape[1]\n","    corr = np.corrcoef(saliency.swapaxes(0, 1).reshape(latent_dim, -1))\n","\n","    print(corr)\n","    return off_diagonal_sum(corr) / (latent_dim * (latent_dim - 1))\n","\n","def pearson_saliency_tensor(saliency):\n","    \"\"\"\n","    Computes the average Pearson correlation between different saliency maps with gradients\n","    Args:\n","        saliency: saliency maps stacked together (indexed by the first tensor dimension)\n","\n","    Returns:\n","        Pearson correlation between saliency maps\n","    \"\"\"\n","    latent_dim = saliency.shape[1]\n","    corr = torch.corrcoef(torch.reshape(torch.swapaxes(saliency, 0, 1), (latent_dim, -1)))\n","    print(torch.reshape(torch.swapaxes(saliency, 0, 1), (latent_dim, -1)))\n","    print(corr)\n","\n","    return (torch.sum(corr) - torch.trace(corr)) / (latent_dim * (latent_dim - 1))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.60455571 0.6439361 ]\n"," [0.60455571 1.         0.62773663]\n"," [0.6439361  0.62773663 1.        ]]\n"]},{"data":{"text/plain":["0.6254094780191829"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["pearson_saliency(saliency_array)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<UnsafeViewBackward0>)\n","tensor([[1.0000, 0.5811, 0.5977],\n","        [0.5811, 1.0000, 0.6213],\n","        [0.5977, 0.6213, 1.0000]], grad_fn=<ClampBackward1>)\n"]},{"data":{"text/plain":["tensor(0.6001, grad_fn=<DivBackward0>)"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["pearson_saliency_tensor(saliency_tensor)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.]])"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["t = torch.tensor([[1.,1.,1.,1.,1.,1.,1.,1.], [1.,1.,1.,1.,1.,1.,1.,1.], [1.,1.,1.,1.,1.,1.,1.,1.]])\n","t"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"text/plain":["array([[nan, nan, nan],\n","       [nan, nan, nan],\n","       [nan, nan, nan]])"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["np.corrcoef(t)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[nan, nan, nan],\n","        [nan, nan, nan],\n","        [nan, nan, nan]])"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["torch.corrcoef(t)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["from scipy.stats import entropy\n","def entropy_saliency(saliency: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Computes the entropy of different saliency maps\n","    Args:\n","        saliency: saliency maps stacked together (indexed by the first tensor dimension)\n","\n","    Returns:\n","        Entropy between saliency maps\n","    \"\"\"\n","    latent_dim = saliency.shape[1]\n","    saliency_reshaped = np.reshape(saliency.swapaxes(1, -1), (-1, latent_dim))\n","    salient_pixels = saliency_reshaped.sum(1) > 0\n","\n","    saliency_filtered = saliency_reshaped[salient_pixels]\n","\n","    entropy_ar = entropy(saliency_filtered, axis=1)\n","\n","    return np.mean(entropy_ar)"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["from torch.distributions import Categorical\n","def entropy_saliency_tensor(saliency: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Computes the entropy of different saliency maps\n","    Args:\n","        saliency: saliency maps stacked together (indexed by the first tensor dimension)\n","\n","    Returns:\n","        Entropy between saliency maps\n","    \"\"\"\n","    latent_dim = saliency.shape[1]\n","    saliency_reshaped = torch.reshape(torch.swapaxes(saliency, 1, -1), (-1, latent_dim))\n","    salient_pixels = saliency_reshaped.sum(1) > 0\n","\n","    saliency_filtered = saliency_reshaped[salient_pixels]\n","\n","    entropy_ar = Categorical(probs = saliency_filtered).entropy()\n","\n","    #entropy_ar = entropy(saliency_filtered, axis=1)\n","    return torch.mean(entropy_ar)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"data":{"text/plain":["1.0986122"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["entropy_saliency(saliency_array)"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.0986, grad_fn=<MeanBackward0>)"]},"execution_count":118,"metadata":{},"output_type":"execute_result"}],"source":["entropy_saliency_tensor(saliency_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMdpCNMZ5YvD"},"outputs":[],"source":["def disvae_feature_importance(\n","    random_seed: int = 1,\n","    batch_size: int = 300,\n","    n_plots: int = 20,\n","    #n_runs: int = 5,\n","    n_runs: int = 2,\n","    dim_latent: int = 3,\n","    #n_epochs: int = 100,\n","    n_epochs: int = 5,\n","    #beta_list: list = [1, 5, 10],\n","    beta_list: list = [1],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [EntropyLoss(alpha=100), BetaHLoss(), BtcvaeLoss(is_mss=False, n_data=len(train_dataset))]\n","    metric_list = [\n","        pearson_saliency,\n","        spearman_saliency,\n","        cos_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Spearman Correlation\",\n","        \"Cosine\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Beta\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for beta, loss, run in itertools.product(\n","        beta_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.beta = beta\n","        name = f\"{str(loss)}-vae_beta{beta}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), beta] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names)\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1310527,"status":"ok","timestamp":1702869670095,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"ziu2dJXK6KQx","outputId":"7ad1bf5a-9d64-4432-e027-f9f596dea0bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Now fitting Entropy-vae_beta1_run1\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Model Entropy-vae_beta1_run1 \t Pearson Correlation 0.52\tSpearman Correlation 0.99\tCosine 0.82\tEntropy 0.066\tActive Neurons 1\n","Now fitting Entropy-vae_beta1_run2\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Model Entropy-vae_beta1_run2 \t Pearson Correlation 0.38\tSpearman Correlation 0.99\tCosine 0.73\tEntropy 0.22\tActive Neurons 1\n","Now fitting Beta-vae_beta1_run1\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Model Beta-vae_beta1_run1 \t Pearson Correlation 0.42\tSpearman Correlation 0.99\tCosine 0.73\tEntropy 0.45\tActive Neurons 1.3\n","Now fitting Beta-vae_beta1_run2\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Model Beta-vae_beta1_run2 \t Pearson Correlation 0.33\tSpearman Correlation 0.98\tCosine 0.62\tEntropy 0.74\tActive Neurons 1.3\n","Now fitting TC-vae_beta1_run1\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Model TC-vae_beta1_run1 \t Pearson Correlation 0.31\tSpearman Correlation 0.99\tCosine 0.63\tEntropy 0.74\tActive Neurons 1.3\n","Now fitting TC-vae_beta1_run2\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Model TC-vae_beta1_run2 \t Pearson Correlation 0.31\tSpearman Correlation 0.98\tCosine 0.61\tEntropy 0.72\tActive Neurons 1.3\n"]}],"source":["disvae_feature_importance()"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":482,"status":"ok","timestamp":1702965425746,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"o3GTBTy1h5sS"},"outputs":[],"source":["def entropy_vae(\n","    random_seed: int = 1,\n","    batch_size: int = 300,\n","    n_plots: int = 20,\n","    n_runs: int = 1,\n","    dim_latent: int = 3,\n","    n_epochs: int = 20,\n","    alpha_list: list = [50],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    train_dataset.data, train_dataset.targets = train_dataset.data[[i for i in range(50)]], train_dataset.targets[[i for i in range(50)]]\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    test_dataset.data, test_dataset.targets = test_dataset.data[[i for i in range(50)]], test_dataset.targets[[i for i in range(50)]]\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/entropy_vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [EntropyLoss()]\n","    metric_list = [\n","        pearson_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Alpha\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for alpha, loss, run in itertools.product(\n","        alpha_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.alpha = alpha\n","        name = f\"{str(loss)}-vae_alpha{alpha}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), alpha] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names, loss='entropy')\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2622966,"status":"error","timestamp":1702968051103,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"B93_xB9piDY_","outputId":"1c72e2d0-0ac0-4e06-babf-0238ec805b8e"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?batches/s]"]},{"name":"stdout","output_type":"stream","text":["Now fitting Entropy-vae_alpha50_run1\n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20 \t \n","Train loss 0.892 \t Test loss 0.703 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 2/20 \t \n","Train loss 0.715 \t Test loss 0.836 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 3/20 \t \n","Train loss 0.833 \t Test loss 0.829 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 4/20 \t \n","Train loss 0.834 \t Test loss 0.802 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 5/20 \t \n","Train loss 0.805 \t Test loss 0.742 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 6/20 \t \n","Train loss 0.749 \t Test loss 0.617 \t \n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?batches/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/20 \t \n","Train loss 0.618 \t Test loss 0.595 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 8/20 \t \n","Train loss 0.593 \t Test loss 0.594 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 9/20 \t \n","Train loss 0.589 \t Test loss 0.538 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 10/20 \t \n","Train loss 0.536 \t Test loss 0.448 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 11/20 \t \n","Train loss 0.445 \t Test loss 0.376 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 12/20 \t \n","Train loss 0.374 \t Test loss 0.315 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 13/20 \t \n","Train loss 0.313 \t Test loss 0.087 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 14/20 \t \n","Train loss 0.103 \t Test loss 0.247 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 15/20 \t \n","Train loss 0.249 \t Test loss 0.237 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 16/20 \t \n","Train loss 0.243 \t Test loss 0.158 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 17/20 \t \n","Train loss 0.159 \t Test loss 0.22 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 18/20 \t \n","Train loss 0.212 \t Test loss 0.21 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                  "]},{"name":"stdout","output_type":"stream","text":["Epoch 19/20 \t \n","Train loss 0.203 \t Test loss 0.16 \t \n"]},{"name":"stderr","output_type":"stream","text":["\r"]},{"name":"stdout","output_type":"stream","text":["Epoch 20/20 \t \n","Train loss 0.157 \t Test loss 0.0418 \t \n","Model Entropy-vae_alpha50_run1 \t Pearson Correlation 0.52\tEntropy 0.041\tActive Neurons 1\n"]},{"ename":"IndexError","evalue":"index 0 is out of bounds for dimension 0 with size 0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-e90ea7dc56e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentropy_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-eb206a8fe2fc>\u001b[0m in \u001b[0;36mentropy_vae\u001b[0;34m(random_seed, batch_size, n_plots, n_runs, dim_latent, n_epochs, alpha_list)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Plot a couple of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         plot_idx = [\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_plots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-eb206a8fe2fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Plot a couple of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         plot_idx = [\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_plots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         ]\n","\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"]}],"source":["entropy_vae()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702979801465,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"cqoeN4Ohh5ui"},"outputs":[],"source":["def pearson_vae(\n","    random_seed: int = 1,\n","    batch_size: int = 1,\n","    n_plots: int = 20,\n","    n_runs: int = 1,\n","    dim_latent: int = 3,\n","    n_epochs: int = 100,\n","    alpha_list: list = [1],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    train_dataset.data, train_dataset.targets = train_dataset.data[[i for i in range(50)]], train_dataset.targets[[i for i in range(50)]]\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    test_dataset.data, test_dataset.targets = test_dataset.data[[i for i in range(50)]], test_dataset.targets[[i for i in range(50)]]\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/pearson_vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [PearsonLoss()]\n","    metric_list = [\n","        pearson_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Alpha\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for alpha, loss, run in itertools.product(\n","        alpha_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.alpha = alpha\n","        name = f\"{str(loss)}-vae_alpha{alpha}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), alpha] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names, loss='entropy')\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":825},"executionInfo":{"elapsed":3664,"status":"error","timestamp":1702979808635,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"rZGVOjZuyxrT","outputId":"a6e2bbf7-829c-42c7-82d6-2837c687f21d"},"outputs":[{"name":"stderr","output_type":"stream","text":["  2%|‚ñè         | 1/50 [00:00<00:08,  5.87batches/s]"]},{"name":"stdout","output_type":"stream","text":["Now fitting Pearson-vae_alpha1_run1\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:01, 44.18batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100 \t \n","Train loss 433 \t Test loss 289 \t \n"]},{"name":"stderr","output_type":"stream","text":["  8%|‚ñä         | 4/50 [00:00<00:01, 39.14batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 2/100 \t \n","Train loss 279 \t Test loss 273 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 47.81batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 3/100 \t \n","Train loss 265 \t Test loss 268 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 47.88batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 4/100 \t \n","Train loss 264 \t Test loss 271 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 47.55batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 5/100 \t \n","Train loss 260 \t Test loss 269 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 47.46batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 6/100 \t \n","Train loss 260 \t Test loss 269 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 46.97batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 7/100 \t \n","Train loss 263 \t Test loss 271 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 47.63batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 8/100 \t \n","Train loss 258 \t Test loss 268 \t \n"]},{"name":"stderr","output_type":"stream","text":["  6%|‚ñå         | 3/50 [00:00<00:01, 24.27batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 9/100 \t \n","Train loss 257 \t Test loss 271 \t \n"]},{"name":"stderr","output_type":"stream","text":[" 10%|‚ñà         | 5/50 [00:00<00:00, 46.28batches/s] "]},{"name":"stdout","output_type":"stream","text":["Epoch 10/100 \t \n","Train loss 258 \t Test loss 270 \t \n"]},{"name":"stderr","output_type":"stream","text":["                                                    \r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2c1389a3f655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpearson_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-c6828778bfbf>\u001b[0m in \u001b[0;36mpearson_vae\u001b[0;34m(random_seed, batch_size, n_plots, n_runs, dim_latent, n_epochs, alpha_list)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Now fitting {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lfxai/models/images.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, device, train_loader, test_loader, save_dir, n_epoch, patience)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;31m#logging.info(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0;31m#    f\"Epoch {epoch + 1}/{n_epoch} \\t \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lfxai/models/images.py\u001b[0m in \u001b[0;36mtest_epoch\u001b[0;34m(self, device, dataloader)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"entropy\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'pearson'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                     loss = self.loss_f(\n\u001b[0m\u001b[1;32m    954\u001b[0m                         \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lfxai/models/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, recon_batch, latent_dist, is_train, storer, encoder, latent_sample)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mbaseline_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mgradshap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientShap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpearson_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pearson_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradshap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lfxai/models/losses.py\u001b[0m in \u001b[0;36m_pearson_loss\u001b[0;34m(encoder, dim_latent, data, device, gradshap, baseline_image)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pearson_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradshap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_attribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradshap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0mpearson_correlation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpearson_saliency_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lfxai/explanations/features.py\u001b[0m in \u001b[0;36mtensor_attribution\u001b[0;34m(encoder, dim_latent, data_loader, device, attr_method, baseline)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mlatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mattribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mattributions_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mattributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/attr/_core/gradient_shap.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, n_samples, stdevs, target, additional_forward_args, return_convergence_delta)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# NOTE: using attribute.__wrapped__ to not log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         attributions = nt.attribute.__wrapped__(\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/attr/_core/noise_tunnel.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, nt_type, nt_samples, nt_samples_batch_size, stdevs, draw_baseline_from_distrib, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m                     \u001b[0mis_attrib_tuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0mdelta_partial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 ) = compute_partial_attribution(inputs_with_noise, kwargs_copy)\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_attributions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/attr/_core/noise_tunnel.py\u001b[0m in \u001b[0;36mcompute_partial_attribution\u001b[0;34m(inputs_with_noise_partition, kwargs_partition)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# NOTE: using __wrapped__ such that it does not log the inner logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             attributions = attr_func.__wrapped__(  # type: ignore\n\u001b[0m\u001b[1;32m    254\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribution_method\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0minputs_with_noise_partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/attr/_core/gradient_shap.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[0;32m--> 366\u001b[0;31m         grads = self.gradient_func(\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_baseline_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         )\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# runs forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         assert outputs[0].numel() == 1, (\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m\"Target not provided when necessary, cannot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     )\n\u001b[0;32m--> 536\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_select_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_select_targets\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["pearson_vae()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702979731373,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"},"user_tz":-660},"id":"xS3fL3kY1CpX","outputId":"138498e6-0092-40b8-8d1a-aefe239cfbec"},"outputs":[{"data":{"text/plain":["tensor(10., requires_grad=True)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["torch.tensor(10.0, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMgQReuzfEIV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM51sKW33h8YYaBJEA9L2K+","gpuType":"T4","mount_file_id":"149iS4S-4jg01oOsFNut57YoK9y3-RvPk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"149iS4S-4jg01oOsFNut57YoK9y3-RvPk","authorship_tag":"ABX9TyPPVQd1CyanKhGhWWG91s0z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install captum\n","!pip install drive/MyDrive/encoder_attribution_priors/."],"metadata":{"id":"3XPm3DUU3yGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5BzCW00v06E"},"outputs":[],"source":["import argparse\n","import csv\n","import itertools\n","import logging\n","import os\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torchvision\n","from captum.attr import GradientShap, IntegratedGradients, Saliency\n","from scipy.stats import spearmanr\n","from torch.utils.data import DataLoader, RandomSampler, Subset\n","from torchvision import transforms\n","\n","from lfxai.explanations.examples import (\n","    InfluenceFunctions,\n","    NearestNeighbours,\n","    SimplEx,\n","    TracIn,\n",")\n","from lfxai.explanations.features import attribute_auxiliary, attribute_individual_dim\n","from lfxai.models.images import (\n","    VAE,\n","    AutoEncoderMnist,\n","    ClassifierMnist,\n","    DecoderBurgess,\n","    DecoderMnist,\n","    EncoderBurgess,\n","    EncoderMnist,\n",")\n","from lfxai.models.losses import BetaHLoss, BtcvaeLoss, EntropyLoss\n","from lfxai.models.pretext import Identity, Mask, RandomNoise\n","from lfxai.utils.datasets import MaskedMNIST\n","from lfxai.utils.feature_attribution import generate_masks\n","from lfxai.utils.metrics import (\n","    compute_metrics,\n","    cos_saliency,\n","    count_activated_neurons,\n","    entropy_saliency,\n","    pearson_saliency,\n","    similarity_rates,\n","    spearman_saliency,\n",")\n","from lfxai.utils.visualize import (\n","    correlation_latex_table,\n","    plot_pretext_saliencies,\n","    plot_pretext_top_example,\n","    plot_vae_saliencies,\n","    vae_box_plots,\n",")"]},{"cell_type":"code","source":["def disvae_feature_importance(\n","    random_seed: int = 1,\n","    batch_size: int = 300,\n","    n_plots: int = 20,\n","    #n_runs: int = 5,\n","    n_runs: int = 2,\n","    dim_latent: int = 3,\n","    #n_epochs: int = 100,\n","    n_epochs: int = 5,\n","    #beta_list: list = [1, 5, 10],\n","    beta_list: list = [1],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [EntropyLoss(alpha=100), BetaHLoss(), BtcvaeLoss(is_mss=False, n_data=len(train_dataset))]\n","    metric_list = [\n","        pearson_saliency,\n","        spearman_saliency,\n","        cos_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Spearman Correlation\",\n","        \"Cosine\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Beta\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for beta, loss, run in itertools.product(\n","        beta_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.beta = beta\n","        name = f\"{str(loss)}-vae_beta{beta}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), beta] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names)\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"],"metadata":{"id":"LMdpCNMZ5YvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["disvae_feature_importance()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziu2dJXK6KQx","executionInfo":{"status":"ok","timestamp":1702869670095,"user_tz":-660,"elapsed":1310527,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"}},"outputId":"7ad1bf5a-9d64-4432-e027-f9f596dea0bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Now fitting Entropy-vae_beta1_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Entropy-vae_beta1_run1 \t Pearson Correlation 0.52\tSpearman Correlation 0.99\tCosine 0.82\tEntropy 0.066\tActive Neurons 1\n","Now fitting Entropy-vae_beta1_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Entropy-vae_beta1_run2 \t Pearson Correlation 0.38\tSpearman Correlation 0.99\tCosine 0.73\tEntropy 0.22\tActive Neurons 1\n","Now fitting Beta-vae_beta1_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Beta-vae_beta1_run1 \t Pearson Correlation 0.42\tSpearman Correlation 0.99\tCosine 0.73\tEntropy 0.45\tActive Neurons 1.3\n","Now fitting Beta-vae_beta1_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Beta-vae_beta1_run2 \t Pearson Correlation 0.33\tSpearman Correlation 0.98\tCosine 0.62\tEntropy 0.74\tActive Neurons 1.3\n","Now fitting TC-vae_beta1_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model TC-vae_beta1_run1 \t Pearson Correlation 0.31\tSpearman Correlation 0.99\tCosine 0.63\tEntropy 0.74\tActive Neurons 1.3\n","Now fitting TC-vae_beta1_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model TC-vae_beta1_run2 \t Pearson Correlation 0.31\tSpearman Correlation 0.98\tCosine 0.61\tEntropy 0.72\tActive Neurons 1.3\n"]}]},{"cell_type":"code","source":["def entropy_vae(\n","    random_seed: int = 1,\n","    batch_size: int = 300,\n","    n_plots: int = 20,\n","    n_runs: int = 5,\n","    dim_latent: int = 3,\n","    n_epochs: int = 100,\n","    alpha_list: list = [1, 5, 10],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/entropy_vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [EntropyLoss()]\n","    metric_list = [\n","        pearson_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Alpha\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for alpha, loss, run in itertools.product(\n","        alpha_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.alpha = alpha\n","        name = f\"{str(loss)}-vae_alpha{alpha}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), alpha] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names)\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"],"metadata":{"id":"o3GTBTy1h5sS","executionInfo":{"status":"ok","timestamp":1702880078272,"user_tz":-660,"elapsed":332,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["entropy_vae()"],"metadata":{"id":"B93_xB9piDY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cqoeN4Ohh5ui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rZGVOjZuyxrT"},"execution_count":null,"outputs":[]}]}
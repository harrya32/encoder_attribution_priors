{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"149iS4S-4jg01oOsFNut57YoK9y3-RvPk","authorship_tag":"ABX9TyM37NwJWRyZPjB4Sp3CiSCP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install captum\n","!pip install drive/MyDrive/encoder_attribution_priors/."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3XPm3DUU3yGb","executionInfo":{"status":"ok","timestamp":1702951249583,"user_tz":-660,"elapsed":29259,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"}},"outputId":"6845b9ff-5b0e-4356-e66a-131ec1175a6a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting captum\n","  Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.7.0\n","Processing ./drive/MyDrive/encoder_attribution_priors\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (0.16.0+cu121)\n","Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (0.7.0)\n","Collecting hydra-core (from lfxai==0.1.1)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (1.5.3)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (0.19.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (1.11.4)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (0.12.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (0.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (4.66.1)\n","Collecting wget (from lfxai==0.1.1)\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lfxai==0.1.1) (3.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->lfxai==0.1.1) (2.1.0)\n","Collecting omegaconf<2.4,>=2.2 (from hydra-core->lfxai==0.1.1)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core->lfxai==0.1.1)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core->lfxai==0.1.1) (23.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lfxai==0.1.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lfxai==0.1.1) (2023.3.post1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lfxai==0.1.1) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lfxai==0.1.1) (2023.12.9)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lfxai==0.1.1) (1.5.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lfxai==0.1.1) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lfxai==0.1.1) (3.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->lfxai==0.1.1) (2.31.0)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core->lfxai==0.1.1) (6.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lfxai==0.1.1) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lfxai==0.1.1) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->lfxai==0.1.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->lfxai==0.1.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->lfxai==0.1.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->lfxai==0.1.1) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lfxai==0.1.1) (1.3.0)\n","Building wheels for collected packages: lfxai, antlr4-python3-runtime, wget\n","  Building wheel for lfxai (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lfxai: filename=lfxai-0.1.1-py3-none-any.whl size=30395 sha256=f23efcbf3e299e233da49bfd2aa47f5422f6cdb3ffa1b7da41ec3be4cb7ff6d9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-og6bc4go/wheels/7b/00/41/6fc986ab56840bbe22a1da3b92c41c9d55e2dcd851163270b7\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=3146d598d7d5fd731b4b3753d39581a92f42acfe6fd566e3356e7028d3544cd6\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=25ab2cad954b2a8ff6b719f9ae3a4a93487e98010dca48b792432c117fcfddfd\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","Successfully built lfxai antlr4-python3-runtime wget\n","Installing collected packages: wget, antlr4-python3-runtime, omegaconf, hydra-core, lfxai\n","Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 lfxai-0.1.1 omegaconf-2.3.0 wget-3.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"z5BzCW00v06E","executionInfo":{"status":"ok","timestamp":1702951266898,"user_tz":-660,"elapsed":13700,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"}}},"outputs":[],"source":["import argparse\n","import csv\n","import itertools\n","import logging\n","import os\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torchvision\n","from captum.attr import GradientShap, IntegratedGradients, Saliency\n","from scipy.stats import spearmanr\n","from torch.utils.data import DataLoader, RandomSampler, Subset\n","from torchvision import transforms\n","\n","from lfxai.explanations.examples import (\n","    InfluenceFunctions,\n","    NearestNeighbours,\n","    SimplEx,\n","    TracIn,\n",")\n","from lfxai.explanations.features import attribute_auxiliary, attribute_individual_dim\n","from lfxai.models.images import (\n","    VAE,\n","    AutoEncoderMnist,\n","    ClassifierMnist,\n","    DecoderBurgess,\n","    DecoderMnist,\n","    EncoderBurgess,\n","    EncoderMnist,\n",")\n","from lfxai.models.losses import BetaHLoss, BtcvaeLoss, EntropyLoss\n","from lfxai.models.pretext import Identity, Mask, RandomNoise\n","from lfxai.utils.datasets import MaskedMNIST\n","from lfxai.utils.feature_attribution import generate_masks\n","from lfxai.utils.metrics import (\n","    compute_metrics,\n","    cos_saliency,\n","    count_activated_neurons,\n","    entropy_saliency,\n","    pearson_saliency,\n","    similarity_rates,\n","    spearman_saliency,\n",")\n","from lfxai.utils.visualize import (\n","    correlation_latex_table,\n","    plot_pretext_saliencies,\n","    plot_pretext_top_example,\n","    plot_vae_saliencies,\n","    vae_box_plots,\n",")"]},{"cell_type":"code","source":["def disvae_feature_importance(\n","    random_seed: int = 1,\n","    batch_size: int = 300,\n","    n_plots: int = 20,\n","    #n_runs: int = 5,\n","    n_runs: int = 2,\n","    dim_latent: int = 3,\n","    #n_epochs: int = 100,\n","    n_epochs: int = 5,\n","    #beta_list: list = [1, 5, 10],\n","    beta_list: list = [1],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [EntropyLoss(alpha=100), BetaHLoss(), BtcvaeLoss(is_mss=False, n_data=len(train_dataset))]\n","    metric_list = [\n","        pearson_saliency,\n","        spearman_saliency,\n","        cos_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Spearman Correlation\",\n","        \"Cosine\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Beta\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for beta, loss, run in itertools.product(\n","        beta_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.beta = beta\n","        name = f\"{str(loss)}-vae_beta{beta}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), beta] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names)\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"],"metadata":{"id":"LMdpCNMZ5YvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["disvae_feature_importance()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziu2dJXK6KQx","executionInfo":{"status":"ok","timestamp":1702869670095,"user_tz":-660,"elapsed":1310527,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"}},"outputId":"7ad1bf5a-9d64-4432-e027-f9f596dea0bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Now fitting Entropy-vae_beta1_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Entropy-vae_beta1_run1 \t Pearson Correlation 0.52\tSpearman Correlation 0.99\tCosine 0.82\tEntropy 0.066\tActive Neurons 1\n","Now fitting Entropy-vae_beta1_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Entropy-vae_beta1_run2 \t Pearson Correlation 0.38\tSpearman Correlation 0.99\tCosine 0.73\tEntropy 0.22\tActive Neurons 1\n","Now fitting Beta-vae_beta1_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Beta-vae_beta1_run1 \t Pearson Correlation 0.42\tSpearman Correlation 0.99\tCosine 0.73\tEntropy 0.45\tActive Neurons 1.3\n","Now fitting Beta-vae_beta1_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model Beta-vae_beta1_run2 \t Pearson Correlation 0.33\tSpearman Correlation 0.98\tCosine 0.62\tEntropy 0.74\tActive Neurons 1.3\n","Now fitting TC-vae_beta1_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model TC-vae_beta1_run1 \t Pearson Correlation 0.31\tSpearman Correlation 0.99\tCosine 0.63\tEntropy 0.74\tActive Neurons 1.3\n","Now fitting TC-vae_beta1_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Model TC-vae_beta1_run2 \t Pearson Correlation 0.31\tSpearman Correlation 0.98\tCosine 0.61\tEntropy 0.72\tActive Neurons 1.3\n"]}]},{"cell_type":"code","source":["def entropy_vae(\n","    random_seed: int = 1,\n","    batch_size: int = 300,\n","    n_plots: int = 20,\n","    n_runs: int = 3,\n","    dim_latent: int = 3,\n","    n_epochs: int = 50,\n","    alpha_list: list = [5],\n",") -> None:\n","    # Initialize seed and device\n","    np.random.seed(random_seed)\n","    torch.random.manual_seed(random_seed)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # Load MNIST\n","    W = 32\n","    img_size = (1, W, W)\n","    data_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/data/mnist\"\n","    train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","    test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","    train_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    test_transform = transforms.Compose([transforms.Resize(W), transforms.ToTensor()])\n","    train_dataset.transform = train_transform\n","    test_dataset.transform = test_transform\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False\n","    )\n","\n","    # Create saving directory\n","    save_dir = Path.cwd() / \"drive/MyDrive/encoder_attribution_priors/experiments/results/mnist/entropy_vae\"\n","    if not save_dir.exists():\n","\n","        print(f\"Creating saving directory {save_dir}\")\n","        os.makedirs(save_dir)\n","\n","    # Define the computed metrics and create a csv file with appropriate headers\n","    loss_list = [EntropyLoss()]\n","    metric_list = [\n","        pearson_saliency,\n","        entropy_saliency,\n","        count_activated_neurons,\n","    ]\n","    metric_names = [\n","        \"Pearson Correlation\",\n","        \"Entropy\",\n","        \"Active Neurons\",\n","    ]\n","    headers = [\"Loss Type\", \"Alpha\"] + metric_names\n","    csv_path = save_dir / \"metrics.csv\"\n","    if not csv_path.is_file():\n","        print(f\"Creating metrics csv in {csv_path}\")\n","\n","        with open(csv_path, \"w\") as csv_file:\n","            dw = csv.DictWriter(csv_file, delimiter=\",\", fieldnames=headers)\n","            dw.writeheader()\n","\n","    for alpha, loss, run in itertools.product(\n","        alpha_list, loss_list, range(1, n_runs + 1)\n","    ):\n","        # Initialize vaes\n","        encoder = EncoderBurgess(img_size, dim_latent)\n","        decoder = DecoderBurgess(img_size, dim_latent)\n","        loss.alpha = alpha\n","        name = f\"{str(loss)}-vae_alpha{alpha}_run{run}\"\n","        model = VAE(img_size, encoder, decoder, dim_latent, loss, name=name)\n","        print(f\"Now fitting {name}\")\n","\n","        model.fit(device, train_loader, test_loader, save_dir, n_epochs)\n","        model.load_state_dict(torch.load(save_dir / (name + \".pt\")), strict=False)\n","\n","        # Compute test-set saliency and associated metrics\n","        baseline_image = torch.zeros((1, 1, W, W), device=device)\n","        gradshap = GradientShap(encoder.mu)\n","        attributions = attribute_individual_dim(\n","            encoder.mu, dim_latent, test_loader, device, gradshap, baseline_image\n","        )\n","        metrics = compute_metrics(attributions, metric_list)\n","        results_str = \"\\t\".join(\n","            [f\"{metric_names[k]} {metrics[k]:.2g}\" for k in range(len(metric_list))]\n","        )\n","        print(f\"Model {name} \\t {results_str}\")\n","\n","\n","        # Save the metrics\n","        with open(csv_path, \"a\", newline=\"\") as csv_file:\n","            writer = csv.writer(csv_file, delimiter=\",\")\n","            writer.writerow([str(loss), alpha] + metrics)\n","\n","        # Plot a couple of examples\n","        plot_idx = [\n","            torch.nonzero(test_dataset.targets == (n % 10))[n // 10].item()\n","            for n in range(n_plots)\n","        ]\n","        images_to_plot = [test_dataset[i][0].numpy().reshape(W, W) for i in plot_idx]\n","        fig = plot_vae_saliencies(images_to_plot, attributions[plot_idx])\n","        fig.savefig(save_dir / f\"{name}.pdf\")\n","        plt.close(fig)\n","\n","    fig = vae_box_plots(pd.read_csv(csv_path), metric_names)\n","    fig.savefig(save_dir / \"metric_box_plots.pdf\")\n","    plt.close(fig)"],"metadata":{"id":"o3GTBTy1h5sS","executionInfo":{"status":"ok","timestamp":1702951398458,"user_tz":-660,"elapsed":4,"user":{"displayName":"Harry Amad","userId":"02345983546097412963"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["entropy_vae()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B93_xB9piDY_","outputId":"76fc8e66-408f-49f4-962c-5016f2eac79a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Now fitting Entropy-vae_alpha5_run1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50 \t \n","Train loss 307 \t Test loss 262 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50 \t \n","Train loss 257 \t Test loss 252 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50 \t \n","Train loss 252 \t Test loss 249 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50 \t \n","Train loss 250 \t Test loss 242 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50 \t \n","Train loss 238 \t Test loss 231 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50 \t \n","Train loss 231 \t Test loss 222 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50 \t \n","Train loss 222 \t Test loss 214 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50 \t \n","Train loss 218 \t Test loss 211 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50 \t \n","Train loss 215 \t Test loss 210 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50 \t \n","Train loss 214 \t Test loss 208 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50 \t \n","Train loss 213 \t Test loss 207 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50 \t \n","Train loss 212 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50 \t \n","Train loss 211 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50 \t \n","Train loss 210 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50 \t \n","Train loss 209 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16/50 \t \n","Train loss 209 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17/50 \t \n","Train loss 209 \t Test loss 204 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18/50 \t \n","Train loss 208 \t Test loss 204 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19/50 \t \n","Train loss 208 \t Test loss 204 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20/50 \t \n","Train loss 207 \t Test loss 204 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21/50 \t \n","Train loss 207 \t Test loss 204 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22/50 \t \n","Train loss 207 \t Test loss 203 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23/50 \t \n","Train loss 207 \t Test loss 204 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29/50 \t \n","Train loss 205 \t Test loss 203 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33/50 \t \n","Train loss 205 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 41/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 42/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 43/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 44/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 45/50 \t \n","Train loss 203 \t Test loss 199 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 46/50 \t \n","Train loss 203 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 47/50 \t \n","Train loss 203 \t Test loss 199 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 48/50 \t \n","Train loss 203 \t Test loss 199 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 49/50 \t \n","Train loss 203 \t Test loss 199 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 50/50 \t \n","Train loss 203 \t Test loss 200 \t \n","Model Entropy-vae_alpha5_run1 \t Pearson Correlation 0.29\tEntropy 0.72\tActive Neurons 1.3\n","Now fitting Entropy-vae_alpha5_run2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50 \t \n","Train loss 311 \t Test loss 264 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50 \t \n","Train loss 258 \t Test loss 253 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50 \t \n","Train loss 253 \t Test loss 250 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50 \t \n","Train loss 251 \t Test loss 248 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50 \t \n","Train loss 248 \t Test loss 244 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50 \t \n","Train loss 245 \t Test loss 241 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50 \t \n","Train loss 237 \t Test loss 230 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50 \t \n","Train loss 231 \t Test loss 226 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50 \t \n","Train loss 228 \t Test loss 224 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50 \t \n","Train loss 227 \t Test loss 222 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50 \t \n","Train loss 225 \t Test loss 219 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50 \t \n","Train loss 221 \t Test loss 214 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50 \t \n","Train loss 216 \t Test loss 210 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50 \t \n","Train loss 214 \t Test loss 209 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50 \t \n","Train loss 213 \t Test loss 209 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16/50 \t \n","Train loss 212 \t Test loss 208 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17/50 \t \n","Train loss 211 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18/50 \t \n","Train loss 210 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19/50 \t \n","Train loss 210 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20/50 \t \n","Train loss 209 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21/50 \t \n","Train loss 209 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22/50 \t \n","Train loss 208 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23/50 \t \n","Train loss 208 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24/50 \t \n","Train loss 208 \t Test loss 203 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25/50 \t \n","Train loss 207 \t Test loss 203 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26/50 \t \n","Train loss 207 \t Test loss 203 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27/50 \t \n","Train loss 207 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28/50 \t \n","Train loss 207 \t Test loss 203 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31/50 \t \n","Train loss 206 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32/50 \t \n","Train loss 206 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33/50 \t \n","Train loss 206 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34/50 \t \n","Train loss 205 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39/50 \t \n","Train loss 205 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 41/50 \t \n","Train loss 205 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 42/50 \t \n","Train loss 205 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 43/50 \t \n","Train loss 204 \t Test loss 202 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 44/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 45/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 46/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 47/50 \t \n","Train loss 204 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 48/50 \t \n","Train loss 204 \t Test loss 201 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 49/50 \t \n","Train loss 203 \t Test loss 200 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 50/50 \t \n","Train loss 203 \t Test loss 200 \t \n","Model Entropy-vae_alpha5_run2 \t Pearson Correlation 0.3\tEntropy 0.71\tActive Neurons 1.3\n","Now fitting Entropy-vae_alpha5_run3\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50 \t \n","Train loss 310 \t Test loss 259 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50 \t \n","Train loss 256 \t Test loss 251 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50 \t \n","Train loss 252 \t Test loss 248 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50 \t \n","Train loss 249 \t Test loss 240 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50 \t \n","Train loss 237 \t Test loss 232 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50 \t \n","Train loss 232 \t Test loss 225 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50 \t \n","Train loss 224 \t Test loss 215 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50 \t \n","Train loss 219 \t Test loss 212 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50 \t \n","Train loss 216 \t Test loss 211 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50 \t \n","Train loss 214 \t Test loss 208 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50 \t \n","Train loss 213 \t Test loss 207 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50 \t \n","Train loss 212 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50 \t \n","Train loss 211 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50 \t \n","Train loss 210 \t Test loss 206 \t \n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50 \t \n","Train loss 210 \t Test loss 205 \t \n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 57/200 [00:11<00:27,  5.21batches/s]"]}]},{"cell_type":"code","source":["1"],"metadata":{"id":"cqoeN4Ohh5ui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rZGVOjZuyxrT"},"execution_count":null,"outputs":[]}]}